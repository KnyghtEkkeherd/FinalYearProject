{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ooj/Dev/HKUST/fyp/FinalYearProject/src/cv/.venv/lib/python3.11/site-packages/torchreid/reid/metrics/rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from ultralytics import YOLO\n",
    "from torchreid.reid.utils import FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary: 129 layers, 3,157,200 parameters, 0 gradients, 8.9 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(129, 3157200, 0, 8.8575488)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO(\"yolov8n.pt\")\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/ooj/Dev/HKUST/fyp/FinalYearProject/src/cv/media/bus.jpg: 448x640 4 persons, 1 bus, 48.3ms\n",
      "Speed: 1.5ms preprocess, 48.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Found person at: tensor([133.3593,  72.5784, 198.6200, 181.3386]) with 0.92 confidence\n",
      "Found person at: tensor([ 34.2597,  82.9228,  91.5337, 181.7508]) with 0.86 confidence\n",
      "Found person at: tensor([ 74.2149,  71.5516, 119.7641, 181.7909]) with 0.79 confidence\n",
      "Found person at: tensor([113.3372,  74.2637, 144.6671, 181.3813]) with 0.66 confidence\n"
     ]
    }
   ],
   "source": [
    "results = model(\"media/bus.jpg\")\n",
    "for result in results:\n",
    "  boxes = result.boxes.xyxy\n",
    "  confidences = result.boxes.conf\n",
    "  classes = result.boxes.cls\n",
    "\n",
    "  for box, conf, cls in zip(boxes, confidences, classes):\n",
    "        if int(cls) == 0:\n",
    "            print(f\"Found person at: {box} with {conf:.2f} confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded imagenet pretrained weights from \"/Users/ooj/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "Model: osnet_x1_0\n",
      "- params: 2,193,616\n",
      "- flops: 978,878,352\n"
     ]
    }
   ],
   "source": [
    "extractor = FeatureExtractor(model_name=\"osnet_x1_0\", model_path=None, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/test_embeddings.pkl\", \"rb\") as f:\n",
    "    test_embeddings = pickle.load(f)\n",
    "\n",
    "with open(\"data/test2_embeddings.pkl\", \"rb\") as f:\n",
    "    test2_embeddings = pickle.load(f)\n",
    "\n",
    "with open(\"data/test3_embeddings.pkl\", \"rb\") as f:\n",
    "    test3_embeddings = pickle.load(f)\n",
    "\n",
    "enrolled = {\n",
    "    \"ooj\": test_embeddings,\n",
    "    \"adel\": test2_embeddings,\n",
    "    \"ooj2\": test3_embeddings,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH_THRESHOLD = 0.7\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 68.3ms\n",
      "Speed: 1.3ms preprocess, 68.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 59.8ms\n",
      "Speed: 1.3ms preprocess, 59.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 62.4ms\n",
      "Speed: 2.2ms preprocess, 62.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 46.4ms\n",
      "Speed: 1.4ms preprocess, 46.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 47.0ms\n",
      "Speed: 1.0ms preprocess, 47.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 50.0ms\n",
      "Speed: 1.0ms preprocess, 50.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 cup, 41.4ms\n",
      "Speed: 1.2ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 42.6ms\n",
      "Speed: 1.6ms preprocess, 42.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 cup, 47.4ms\n",
      "Speed: 1.4ms preprocess, 47.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 cup, 46.7ms\n",
      "Speed: 1.1ms preprocess, 46.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 cup, 48.2ms\n",
      "Speed: 1.2ms preprocess, 48.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 46.4ms\n",
      "Speed: 1.1ms preprocess, 46.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 cup, 47.7ms\n",
      "Speed: 1.3ms preprocess, 47.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 cup, 47.0ms\n",
      "Speed: 1.1ms preprocess, 47.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 cup, 47.6ms\n",
      "Speed: 1.2ms preprocess, 47.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 49.6ms\n",
      "Speed: 1.4ms preprocess, 49.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 46.3ms\n",
      "Speed: 1.3ms preprocess, 46.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 50.7ms\n",
      "Speed: 1.1ms preprocess, 50.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 handbag, 1 cup, 51.7ms\n",
      "Speed: 1.4ms preprocess, 51.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 handbag, 1 cup, 48.9ms\n",
      "Speed: 1.2ms preprocess, 48.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 handbag, 1 cup, 45.7ms\n",
      "Speed: 1.2ms preprocess, 45.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 handbag, 1 cup, 49.6ms\n",
      "Speed: 1.1ms preprocess, 49.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 handbag, 1 cup, 45.6ms\n",
      "Speed: 1.1ms preprocess, 45.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 handbag, 1 cup, 43.3ms\n",
      "Speed: 1.2ms preprocess, 43.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 45.6ms\n",
      "Speed: 1.6ms preprocess, 45.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 cup, 58.5ms\n",
      "Speed: 23.4ms preprocess, 58.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 46.5ms\n",
      "Speed: 1.7ms preprocess, 46.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 49.4ms\n",
      "Speed: 1.2ms preprocess, 49.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 50.0ms\n",
      "Speed: 1.1ms preprocess, 50.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 49.4ms\n",
      "Speed: 1.4ms preprocess, 49.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 52.0ms\n",
      "Speed: 1.1ms preprocess, 52.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 handbag, 52.1ms\n",
      "Speed: 1.0ms preprocess, 52.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 52.7ms\n",
      "Speed: 1.4ms preprocess, 52.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 48.8ms\n",
      "Speed: 1.1ms preprocess, 48.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 53.6ms\n",
      "Speed: 1.1ms preprocess, 53.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 44.8ms\n",
      "Speed: 1.1ms preprocess, 44.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 43.9ms\n",
      "Speed: 1.1ms preprocess, 43.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 49.3ms\n",
      "Speed: 1.2ms preprocess, 49.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 46.7ms\n",
      "Speed: 1.2ms preprocess, 46.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 54.6ms\n",
      "Speed: 1.3ms preprocess, 54.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 56.4ms\n",
      "Speed: 1.3ms preprocess, 56.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 51.7ms\n",
      "Speed: 1.8ms preprocess, 51.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 49.3ms\n",
      "Speed: 1.2ms preprocess, 49.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 45.3ms\n",
      "Speed: 1.3ms preprocess, 45.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 49.8ms\n",
      "Speed: 1.2ms preprocess, 49.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 49.0ms\n",
      "Speed: 1.3ms preprocess, 49.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 45.0ms\n",
      "Speed: 1.2ms preprocess, 45.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 handbag, 45.9ms\n",
      "Speed: 1.1ms preprocess, 45.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 handbag, 48.4ms\n",
      "Speed: 1.4ms preprocess, 48.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 handbag, 46.3ms\n",
      "Speed: 1.3ms preprocess, 46.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 43.2ms\n",
      "Speed: 1.1ms preprocess, 43.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 53.4ms\n",
      "Speed: 1.3ms preprocess, 53.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 45.4ms\n",
      "Speed: 1.1ms preprocess, 45.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 50.0ms\n",
      "Speed: 1.3ms preprocess, 50.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 52.9ms\n",
      "Speed: 1.7ms preprocess, 52.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 chair, 48.5ms\n",
      "Speed: 1.4ms preprocess, 48.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 chair, 48.8ms\n",
      "Speed: 1.3ms preprocess, 48.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 45.9ms\n",
      "Speed: 1.3ms preprocess, 45.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 50.8ms\n",
      "Speed: 1.5ms preprocess, 50.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 57.0ms\n",
      "Speed: 1.1ms preprocess, 57.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 chair, 46.7ms\n",
      "Speed: 1.3ms preprocess, 46.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 chair, 47.9ms\n",
      "Speed: 1.3ms preprocess, 47.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 chair, 44.9ms\n",
      "Speed: 1.3ms preprocess, 44.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 45.2ms\n",
      "Speed: 1.6ms preprocess, 45.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 50.0ms\n",
      "Speed: 1.2ms preprocess, 50.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 44.5ms\n",
      "Speed: 1.1ms preprocess, 44.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 handbag, 48.1ms\n",
      "Speed: 1.3ms preprocess, 48.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 48.3ms\n",
      "Speed: 1.1ms preprocess, 48.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 45.9ms\n",
      "Speed: 1.6ms preprocess, 45.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 48.5ms\n",
      "Speed: 1.2ms preprocess, 48.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 52.5ms\n",
      "Speed: 1.4ms preprocess, 52.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 46.5ms\n",
      "Speed: 1.1ms preprocess, 46.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 48.8ms\n",
      "Speed: 1.3ms preprocess, 48.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 47.3ms\n",
      "Speed: 1.1ms preprocess, 47.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 45.7ms\n",
      "Speed: 1.1ms preprocess, 45.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 46.2ms\n",
      "Speed: 1.2ms preprocess, 46.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 51.3ms\n",
      "Speed: 1.1ms preprocess, 51.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        time.sleep(0.01)\n",
    "        continue\n",
    "    \n",
    "    results = model(frame)\n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy \n",
    "        classes = result.boxes.cls \n",
    "        \n",
    "        for box, cls in zip(boxes, classes):\n",
    "            if int(cls) != 0:  # only ppl\n",
    "                continue\n",
    "\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            roi = frame[y1:y2, x1:x2]\n",
    "            \n",
    "            roi_embedding_tensor = extractor([roi])\n",
    "            roi_embedding = roi_embedding_tensor.cpu().numpy().flatten()\n",
    "            norm = np.linalg.norm(roi_embedding)\n",
    "            roi_embedding = roi_embedding if norm == 0 else roi_embedding / norm\n",
    "            \n",
    "            best_score = -1.0\n",
    "            best_label = \"Unknown\"\n",
    "            \n",
    "            for label, embedding in enrolled.items():\n",
    "                for enrolled_emb in embedding:\n",
    "                    score = np.dot(roi_embedding, enrolled_emb)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_label = label\n",
    "            \n",
    "            if best_score < MATCH_THRESHOLD:\n",
    "                best_label = \"Unknown\"\n",
    "            \n",
    "            # Draw the bounding box and label on the frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "            cv2.putText(frame, f\"{best_label} ({best_score:.2f})\", \n",
    "                        (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOCAL_LENGTH_PIXELS = 500\n",
    "KNOWN_PERSON_HEIGHT = 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    results = model(frame)\n",
    "    \n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy\n",
    "        classes = result.boxes.cls\n",
    "        for box, cls in zip(boxes, classes):\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                bbox_height = y2 - y1\n",
    "                distance = (FOCAL_LENGTH_PIXELS * KNOWN_PERSON_HEIGHT) / bbox_height\n",
    "                \n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"Dist: {distance:.2f}m\", (x1, y1 - 10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Distance Estimation\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
